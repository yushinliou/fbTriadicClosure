# -*- coding: utf-8 -*-
"""alter_Var.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Qt64amD6NKSZFZ3T8pmnPPjSRlA6I50g
"""

# created by Yu-Shin 20221113
# last adjust: 20221113
import os
os.system('pip install swifter')
import swifter
import csv
import json
from google.colab import drive
import pandas as pd
import numpy as np
from glob import glob
import re
import time
import numexpr
import datetime as dtt
from datetime import datetime
import argparse
from pathlib import Path

"""# args"""

def parse_args():
  parser = argparse.ArgumentParser(
      description='compute online var base on all tie and firstday'
  )
  parser.add_argument(
      '--ch_dir',
      type=str,
      default='/content/drive/MyDrive/fb_college/traid'
  )
  parser.add_argument(
      '--dtype_file',
      type=str,
      default='./tmp/dtype.json'
  )
  parser.add_argument(
      '--parse_dates_file',
      type=str,
      default='./tmp/parse_dates.json'
  )
  parser.add_argument(
      '--time_range',
      type=list,
      help='research time range',
      default=['2012-09-30', '2015-06-30'],
  )
  parser.add_argument(
      '--res_filt',
      type=float,
      help='filt response rate',
      default=0.7,
  )
  parser.add_argument(
      '--tie_mode',
      type=str,
      help='tie type, if departclass then only analysis in class tie',
      default='departclass',
  )
  parser.add_argument(
      '--var_level',
      type=str,
      help='compute tie level, can chose ego, d1 or d2',
      default='d1',
  )
  # --------------------------> raw data file
  parser.add_argument(
      '--survey_file',
      type=str,
      help='raw survey file',
      default='../raw/csv/survey.csv'
  )
  # --------------------------> preprocess file
  parser.add_argument(
      '--all_tie_file',
      type=str,
      default='./tmp/all_tie_departclass.csv',
  )
  parser.add_argument(
      '--egolist_file',
      type=str,
      help='research ego list, default is from the class reponse rate > 0.7',
      default='./tmp/egores0.7.csv'
  )
  parser.add_argument(
      '--egofirstime_file',
      type=str,
      help='firstime ego use face book',
      default='./tmp/ego_firstime.csv'
  )
  parser.add_argument(
      '--d1firstday_file',
      type=str,
      help='d1 first time',
      default='./tmp/d1firstday_departclass.csv'
  )
  parser.add_argument(
      '--d2firstday_file',
      type=str,
      help='d1 first time',
      default='./tmp/d2firstday_departclass.csv'
  )
  parser.add_argument(
      '--d2d1ego_file',
      type=str,
      help='d2-d1 level contain their ego',
      default='./tmp/d2d1ego_departclass.csv'
  )
  parser.add_argument(
      '--output_dir',
      type=str,
      help='d2-d1 level contain their ego',
      default='./tmp/',
  )
  parser.add_argument(
      '--isDebug',
      type=bool,
      help='if true, only read 100 line',
      default=False,
  )
  parser.add_argument(
      '--debug_n',
      type=int,
      help='if in debug mode read n rows',
      default=100,
  )
  args, unknown = parser.parse_known_args()
  return args
args = parse_args()

# set up working dir
os.chdir(args.ch_dir)
# from utils import onlineVar # 自定義自定義package
# set up dataframe display format
# read dtype and time columns file
dtype = json.loads(Path(args.dtype_file).read_text())
parse_dates = json.loads(Path(args.parse_dates_file).read_text())

"""# read file"""

# tie file
usecols = ['from_id_p', 'from_id', 'createdtime_date',
           'tie_type', 'tie_file', 'tag_or_com', 'from_id_departclass', 'from_id_p_departclass']
parse_dates_col = [parse_dates[args.all_tie_file.split('/')[-1]]] # 取出檔名，查詢這個檔名的時間欄位名稱
tie_df = pd.read_csv(args.all_tie_file,
                     dtype=dtype,
                     parse_dates=parse_dates_col,
                     usecols=usecols)
# read alter_df file
if (args.var_level == 'ego') or (args.var_level == 'd2'):
  alter_file = args.d2firstday_file
  usecols = ['ego_id' ,'d2_alterid', 'd2firstday',
           'eventover', 'transferday', 'egofirstime']
elif (args.var_level == 'd1'):
  alter_file = args.d2d1ego_file
  usecols = ['ego_id' ,'d2_alterid', 'd1_alterid', 'd2firstday', 'd1firstday', 'eventover', 'createdtime_date', 'egofirstime']

parse_dates_col = parse_dates[alter_file.split('/')[-1]]

if args.isDebug:
  nrows=args.debug_n
else:
  nrows=None

alter_df = pd.read_csv(alter_file,
                    parse_dates=parse_dates_col,
                    dtype=dtype,
                    usecols=usecols,
                    nrows=nrows)

"""# def function"""

def action_ct(id, start, end, action_dt):
  try:
    act_ct = action_dt.loc[id].loc[start:end].shape[0]
    return act_ct
  except:
    return 0

def in_action_ct(dt, act_ct_col, id_col, start_col, end_col, act_dt):
  dt[act_ct_col] = dt.swifter.apply(
      lambda row: action_ct(row[id_col],
                          row[start_col], row[end_col], act_dt),
                          axis=1)
  return dt

def tie_preprocess(tie_df, alter_df, alter_id, splits, all_tie_dic, var_duration):

  split_tie = dict()

  # deal withe time duplicated
  time_col = all_tie_dic['time']
  time_str = time_col + '_str'

  tie_df[time_str] = tie_df[time_col]
  print(tie_df.shape[0])
  subset_cols = all_tie_dic['subset_cols']
  subset_cols.append(time_col)
  print(subset_cols)
  tie_df.drop_duplicates(subset=all_tie_dic['subset_cols'])

  
  for split, id in zip(splits, all_tie_dic['tie_cols']):
    print(split, id)
    # only keep comment/tag record from id in alter_df
    alter_df = alter_df[[alter_id]]
    alter_df = alter_df.drop_duplicates()
    # print(list_dt.shape[0]) all_tie_dic[split]['tie']
    tmp_df = tie_df.merge(alter_df,
                          left_on=[id], right_on=[alter_id],
                          how='right', indicator=True)
    # print(dt, dt.shape[0])
    tmp_df = tmp_df[tmp_df['_merge'] != 'right_only']
    # gen a copy of form_id and from_id_p before set them in index
    tmp_df[str(id) + '_copy'] = tmp_df[id]
    # drop loop tie
    act_id = all_tie_dic['tie_cols'][0]
    passive_id = all_tie_dic['tie_cols'][1]
    tmp_df = tmp_df[tmp_df[act_id] != tmp_df[passive_id]]

    # set time index
    tmp_df = tmp_df.sort_values(by=[id, time_col])
    tmp_df.set_index([id,
                  pd.DatetimeIndex(tmp_df[time_col])], inplace=True)
    
    split_tie[split] = tmp_df

  return split_tie

SPLITS = ['com', 'tag']
all_tie_dic = {'time': 'createdtime_date',
          'subset_cols': ['from_id', 'from_id_p', 'tag_or_com'],
          'tie_cols': ['from_id', 'from_id_p'],
          }
if (args.var_level == 'd1'):
  alter_id = 'd1_alterid'
  var_duration = ['d1firstday', 'eventover']
elif (args.var_level == 'd2'):
  alter_id = 'd2_alterid'
  var_duration = ['d2firstday', 'eventover']
elif (args.var_level == 'ego'):
  alter_id = 'ego_id'
  var_duration = ['egofirstime', 'd2firstday']

splits_tie = tie_preprocess(tie_df, alter_df, alter_id, SPLITS, all_tie_dic, var_duration)

for split in splits_tie:
  splits_tie[split]
  new_act_col = f'{args.var_level}_{split}'
  act_ct_df = in_action_ct(alter_df, new_act_col, alter_id,
                                      var_duration[0], var_duration[1],
                                      splits_tie[split])
  act_ct_df.to_csv(f'{args.output_dir}{args.var_level}_{split}.csv', index=False)

