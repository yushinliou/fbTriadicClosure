{"cells":[{"cell_type":"markdown","id":"1a65d72c-ba90-45c6-a9a9-495bdc3a7765","metadata":{"id":"1a65d72c-ba90-45c6-a9a9-495bdc3a7765"},"source":["# ego_firstday\n","整合所有資料的time，找出ego使用臉書的最小值 = ego_firstday"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yUr4WtyDXdIA","executionInfo":{"status":"ok","timestamp":1668335050849,"user_tz":-480,"elapsed":2624,"user":{"displayName":"Sinica Social Simulation","userId":"01424007479022510651"}},"outputId":"9f370243-9cc6-409e-aecf-5b5d71c13fe5"},"id":"yUr4WtyDXdIA","execution_count":70,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","execution_count":71,"id":"f6f65d7c-0c9f-46ee-b3b5-65c207e2bf16","metadata":{"id":"f6f65d7c-0c9f-46ee-b3b5-65c207e2bf16","executionInfo":{"status":"ok","timestamp":1668335050850,"user_tz":-480,"elapsed":5,"user":{"displayName":"Sinica Social Simulation","userId":"01424007479022510651"}}},"outputs":[],"source":["# import 所需套件\n","# import igraph as ig\n","import numpy as np\n","import csv\n","import os\n","import datetime as dt\n","from glob import glob\n","import pandas as pd\n","import argparse"]},{"cell_type":"code","source":["def parse_args():\n","  parser = argparse.ArgumentParser(\n","      description='combine all first day and generate survival dataform'\n","  )\n","  parser.add_argument(\n","      '--ch_dir',\n","      type=str,\n","      default='/content/drive/MyDrive/fb_college/traid'\n","  )\n","  parser.add_argument(\n","      '--raw_dir',\n","      type=str,\n","      default='../raw/csv/'\n","  )\n","  parser.add_argument(\n","      '--dtype_file',\n","      type=str,\n","      default='./tmp/dtype.json'\n","  )\n","  parser.add_argument(\n","      '--parse_dates_file',\n","      type=str,\n","      default='./tmp/parse_dates.json'\n","  )\n","  parser.add_argument(\n","      '--time_range',\n","      type=list,\n","      help='research time range',\n","      default=['2012-09-30', '2015-06-30'],\n","  )\n","  parser.add_argument(\n","      '--res_filt',\n","      type=float,\n","      help='filt response rate',\n","      default=0.7,\n","  )\n","  parser.add_argument(\n","      '--tie_mode',\n","      type=str,\n","      help='tie type, if departclass then only analysis in class tie',\n","      default='departclass',\n","  )\n","  # --------------------------> raw data file\n","  parser.add_argument(\n","      '--survey_file',\n","      type=str,\n","      help='raw survey file',\n","      default='../raw/csv/survey.csv'\n","  )\n","  # --------------------------> preprocess file\n","  parser.add_argument(\n","      '--all_tie_file',\n","      type=str,\n","      default='./tmp/all_tie_departclass.csv',\n","  )\n","  parser.add_argument(\n","      '--egolist_file',\n","      type=str,\n","      help='research ego list, default is from the class reponse rate > 0.7',\n","      default='./tmp/egores0.7.csv'\n","  )\n","  parser.add_argument(\n","      '--egofirstime_file',\n","      type=str,\n","      help='firstime ego use face book',\n","      default='./tmp/ego_firstime.csv'\n","  )\n","  parser.add_argument(\n","      '--d1firstday_file',\n","      type=str,\n","      help='d1 first time',\n","      default='./tmp/d1firstday_departclass.csv'\n","  )\n","  parser.add_argument(\n","      '--d2d1ego_file',\n","      type=str,\n","      help='d2-d1 level contain their ego',\n","      default='./tmp/d2d1ego_departclass.csv'\n","  )\n","  parser.add_argument(\n","      '--output_dir',\n","      type=str,\n","      help='d2-d1 level contain their ego',\n","      default='./tmp/ego_firstime.csv',\n","  )\n","  parser.add_argument(\n","      '--write_dta',\n","      type=bool,\n","      help='whether to write dta stata file or not',\n","      default=False,\n","  )\n","  args, unknown = parser.parse_known_args()\n","  return args\n","args = parse_args()"],"metadata":{"id":"x-U8fMkPWb4b","executionInfo":{"status":"ok","timestamp":1668335050850,"user_tz":-480,"elapsed":4,"user":{"displayName":"Sinica Social Simulation","userId":"01424007479022510651"}}},"id":"x-U8fMkPWb4b","execution_count":72,"outputs":[]},{"cell_type":"code","source":["os.chdir(args.ch_dir) # 切換目錄切換目錄"],"metadata":{"id":"fbb5y6NYXWkR","executionInfo":{"status":"ok","timestamp":1668335050850,"user_tz":-480,"elapsed":3,"user":{"displayName":"Sinica Social Simulation","userId":"01424007479022510651"}}},"id":"fbb5y6NYXWkR","execution_count":73,"outputs":[]},{"cell_type":"code","source":["# 找出來目標要合併的欄位名稱跟檔名\n","find = 0\n","ct = 0\n","columns = []\n","target_file_list = [] # 預計要抓出來找最早時間的檔案\n","target_format_list = []\n","time_cols = ['created_time_date', 'created_time'] # 已經確認只有這兩種時間型態\n"," \n","# 開啟所有目錄下的 CSV 檔案\n","files = os.listdir(args.raw_dir) # 確認目錄內容"],"metadata":{"id":"y4tJIK4hXoTX","executionInfo":{"status":"ok","timestamp":1668335050850,"user_tz":-480,"elapsed":3,"user":{"displayName":"Sinica Social Simulation","userId":"01424007479022510651"}}},"id":"y4tJIK4hXoTX","execution_count":74,"outputs":[]},{"cell_type":"code","execution_count":75,"id":"2f9c1aa5-5109-4910-8ebf-7eb57ea32e60","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2f9c1aa5-5109-4910-8ebf-7eb57ea32e60","executionInfo":{"status":"ok","timestamp":1668335052332,"user_tz":-480,"elapsed":1485,"user":{"displayName":"Sinica Social Simulation","userId":"01424007479022510651"}},"outputId":"399290d7-9940-4d5f-e1be-06fbe53db2a0"},"outputs":[{"output_type":"stream","name":"stdout","text":["file we ready to find minum creatime []\n"]}],"source":["files.remove('survey.csv')\n","files.remove('events.csv')\n","\n","# 做兩個串列，分別紀錄日期欄位名稱叫做\"created_time_date\"跟\"created_time\"\n","for f in files:\n","  f_path = args.raw_dir + f\n","  # 讀檔案，只讀一行（標題欄位）\n","  df = pd.read_csv(f_path, nrows=1)\n","  # 欄位名稱轉成字串\n","  columns = list(df.columns)\n","  # 遍歷每一個欄位\n","  for column in columns:\n","    # 兩種時間欄位的名稱\n","    for time_col in time_cols:\n","      # 如果有跟這個檔案欄位名稱相符合\n","      if column == time_col:\n","        # 合併到要處理的檔案裡面\n","        # f.append(time_col)\n","        target_file_list.append(f) # 記下檔名\n","        target_file_list.append(time_col) # 記下欄位名稱\n","        find = 1\n","  # 如果這個檔案裡面有時間欄位\n","  if find == 1:\n","    target_format_list.append(target_file_list) # 把欄位名稱跟檔名記下來\n","    find = 0 # 歸零、字串清空等待下一輪搜索\n","    target_file_list = []\n","\n","print(\"file we ready to find minum creatime\", target_file_list)"]},{"cell_type":"code","source":["# 執行時間一分鐘\n","dtype_dic = {'OwnerFbid':str}\n","rename_dic = {'created_time': 'egofirstday',\n","              'created_time_date': 'egofirstday'}\n","\n","for i in range(len(target_format_list)): # 遍歷每一個有時間的檔案\n","\n","  # read file\n","  usecols = [target_format_list[i][1], 'OwnerFbid'] # 只讀取時間欄位跟Ownerfbid\n","  parse_dates = [target_format_list[i][1]]\n","  f_path = args.raw_dir + target_format_list[i][0]\n","  df = pd.read_csv(f_path, index_col=None,\n","                   usecols=usecols,\n","                   header=0, dtype=dtype_dic,\n","                   parse_dates=parse_dates)\n","  \n","  # 設定時間範圍，去除掉臉書進駐台灣以前的資料臉書進駐台灣以前的資料\n","  time_col_str = str(target_format_list[i][1])\n","  mask = (df[time_col_str] >= '2008-6-20')\n","  df = df[mask]\n","  # 按日期排列\n","  df = df.sort_values(by=time_col_str)\n","  # 去除掉重複的，只留下最早的\n","  df.drop_duplicates(subset=['OwnerFbid'], inplace=True, ignore_index=True)\n","  # 在第一圈迴圈的地方，記下來當前的資料egofirstday\n","  if i == 0:\n","    egofirstday_df = df\n","  # 後面的迴圈後面的迴圈append egofirstday\n","  else:\n","    egofirstday_df.append(df)\n","  \n","# 最後再找真正的所有檔案裡面的所有檔案裡面的egofirstday\n","egofirstday_df = egofirstday_df.sort_values(by='created_time')\n","# 去除掉重複的，只留下最早的\n","egofirstday_df.drop_duplicates(subset=['OwnerFbid'], inplace=True, ignore_index=True)"],"metadata":{"id":"WPG2E_v9q6GA","executionInfo":{"status":"ok","timestamp":1668335115535,"user_tz":-480,"elapsed":63207,"user":{"displayName":"Sinica Social Simulation","userId":"01424007479022510651"}}},"id":"WPG2E_v9q6GA","execution_count":76,"outputs":[]},{"cell_type":"code","source":["# 轉換時區到台北時間\n","def tz_drop(dt, time_col):\n","  dt[time_col] = dt[time_col].dt.tz_convert('Asia/Taipei')\n","  # 再去除掉時區\n","  dt[time_col] = dt[time_col].dt.tz_localize(None)\n","  return dt"],"metadata":{"id":"l7aXY83hHdUz","executionInfo":{"status":"ok","timestamp":1668335115537,"user_tz":-480,"elapsed":21,"user":{"displayName":"Sinica Social Simulation","userId":"01424007479022510651"}}},"id":"l7aXY83hHdUz","execution_count":77,"outputs":[]},{"cell_type":"code","source":["dta_filename = args.egofirstime_file.split('c')[0] + 'dta'"],"metadata":{"id":"ZqsMadbfgpDl","executionInfo":{"status":"ok","timestamp":1668335115537,"user_tz":-480,"elapsed":16,"user":{"displayName":"Sinica Social Simulation","userId":"01424007479022510651"}}},"id":"ZqsMadbfgpDl","execution_count":78,"outputs":[]},{"cell_type":"code","source":["# convert time zone to Taipeitime\n","egofirstday_d = tz_drop(egofirstday_df, 'created_time')\n","egofirstday_df = egofirstday_df.rename(columns={'created_time': 'egofirstime'})\n","if args.write_dta:\n","  dta_file = args.egofirstime_file.split('c')[0] + 'dta'\n","  egofirstday_df.to_stata(dta_file, write_index=False)\n","  print(f'write {dta_file}')\n","else:\n","  egofirstday_df.to_csv(args.egofirstime_file, index=False)\n","  print(f'write {args.egofirstime_file}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y1gxymqhYa6E","executionInfo":{"status":"ok","timestamp":1668335115538,"user_tz":-480,"elapsed":15,"user":{"displayName":"Sinica Social Simulation","userId":"01424007479022510651"}},"outputId":"c34cb015-3825-4d5c-b3a1-d29bda0724a6"},"id":"y1gxymqhYa6E","execution_count":79,"outputs":[{"output_type":"stream","name":"stdout","text":["write ./tmp/ego_firstime.csv\n"]}]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.2"},"colab":{"provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":5}